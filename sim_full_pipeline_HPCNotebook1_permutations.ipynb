{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SurvivalLCS Experiment Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import glob\n",
    "from datetime import date\n",
    "import argparse\n",
    "from random import shuffle\n",
    "from random import sample\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import shutil\n",
    "import sksurv\n",
    "import pickle\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from survival_LCS_permutations import survivalLCS_permutations as survivalLCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/bandheyh/common/survival-lcs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x15550c87d490>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.ioff()\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survival-LCS Parameters\n",
    "\n",
    "### Set file names and necessary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter to run using hpc resources\n",
    "HPC = True\n",
    "\n",
    "homedir = \"/home/bandheyh/common/survival-lcs/pipeline\"\n",
    "models = ['me', 'epi', 'het', 'add']\n",
    "m0s = []\n",
    "\n",
    "c = [0.1,0.4,0.8]\n",
    "nfeat = ['f100'] #add f10000 when on cluster\n",
    "maf = ['maf0.2','maf0.4']\n",
    "\n",
    "iterations = 50000\n",
    "cv_splits = 5\n",
    "\n",
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    models = ['me']\n",
    "    c = [0.1]\n",
    "    nfeat = ['f100', 'f1000']\n",
    "    maf = ['maf0.2', 'maf0.4']\n",
    "    iterations = 1000\n",
    "    cv_splits = 3\n",
    "\n",
    "### Create empty brier score DataFrame\n",
    "brier_df = pd.DataFrame()\n",
    "cox_brier_df = pd.DataFrame()\n",
    "\n",
    "# other non-parameters\n",
    "\n",
    "simulated = True # CHANGE THIS TO FALSE IF RUNNING REAL DATA\n",
    "\n",
    "lcs_run = True\n",
    "dtype_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the survival_LCS pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from survival_LCS_permutations import survivalLCS_permutations as survivalLCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the survival_LCS pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(models, nfeat, maf, i, j, k):\n",
    "\n",
    "    g = homedir + '/' + 'simulated_datasets/' + \\\n",
    "        'EDM-1_one_of_each/'+str(models[i]) + \\\n",
    "        '_' + str(nfeat[j]) + '_' + str(maf[k]) + '_' + 'EDM-1_01.txt'\n",
    "    dtype = str(models[i]) + '_' + str(nfeat[j]) + '_' + str(maf[k])\n",
    "    dtype_list.append(dtype)\n",
    "    print(g)\n",
    "\n",
    "    d = homedir + '/' + 'cv_sim_data/cv_' + str(models[i]) + '/' + dtype\n",
    "    m = homedir + '/' + 'pickled_cv_models/' + str(models[i]) + '/' + dtype\n",
    "    o = homedir + '/' + 'sim_lcs_output/' + str(models[i]) + '/' + dtype\n",
    "\n",
    "    ### Set m0_path\n",
    "    if models[i] in ['me','add','het']:\n",
    "        m0_path = homedir+'/'+'simulated_datasets/'+'EDM-1_one_of_each/model_files/me_h0.2_'+str(maf[k])+'_Models.txt'\n",
    "    else:\n",
    "        m0_path = homedir+'/'+'simulated_datasets/'+'EDM-1_one_of_each/model_files/epi_h0.2_'+str(maf[k])+'_Models.txt'\n",
    "\n",
    "    ### Set m1_path\n",
    "    if models[i] in ['me','epi']:\n",
    "        m1_path = None\n",
    "    else:\n",
    "        m1_path = homedir+'/'+'simulated_datasets/'+'EDM-1_one_of_each/model_files/epi_h0.2_'+str(maf[k])+'_Models.txt'\n",
    "\n",
    "    ### Set m0_type\n",
    "    if models[i] in ['me','add','het']:\n",
    "        m0_type = 'main_effect'\n",
    "    else:\n",
    "        m0_type = '2way_epistasis'\n",
    "\n",
    "    ### Set m1_type\n",
    "    if models[i] in ['me', 'epi']:\n",
    "        m1_type = None\n",
    "    else:\n",
    "        m1_type = '2way_epistasis'\n",
    "\n",
    "    ### Set mtype\n",
    "    if models[i] == 'me':\n",
    "        mtype = 'main_effect'\n",
    "    elif models[i] == 'epi':\n",
    "        mtype = '2way_epistasis'\n",
    "    elif models[i] == 'add':\n",
    "        mtype = 'additive'\n",
    "    else:\n",
    "        mtype = 'heterogeneous'\n",
    "\n",
    "\n",
    "    e = \"testallsims\"\n",
    "    print(str(models[i])+'_'+str(nfeat[j])+'_'+str(maf[k]))\n",
    "\n",
    "    return g, mtype, d, m, o, e,brier_df,cox_brier_df, m0_path, m0_type, m1_path, m1_type\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_slcs(survivalLCS):\n",
    "\n",
    "    lcs_run = True\n",
    "\n",
    "    if lcs_run == True:\n",
    "        survivalLCS.returnCVModelFiles()\n",
    "        brier_df = survivalLCS.returnIBSresults()\n",
    "\n",
    "    else:\n",
    "        print(\"Datasets generated only\")\n",
    "\n",
    "    return brier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bandheyh/common/survival-lcs/pipeline/simulated_datasets/EDM-1_one_of_each/me_f100_maf0.2_EDM-1_01.txt\n",
      "me_f100_maf0.2\n",
      "None\n",
      "/home/bandheyh/common/survival-lcs/pipeline/simulated_datasets/EDM-1_one_of_each/me_f100_maf0.4_EDM-1_01.txt\n",
      "me_f100_maf0.4\n",
      "None\n",
      "/home/bandheyh/common/survival-lcs/pipeline/simulated_datasets/EDM-1_one_of_each/epi_f100_maf0.2_EDM-1_01.txt\n",
      "epi_f100_maf0.2\n",
      "None\n",
      "/home/bandheyh/common/survival-lcs/pipeline/simulated_datasets/EDM-1_one_of_each/epi_f100_maf0.4_EDM-1_01.txt\n",
      "epi_f100_maf0.4\n",
      "None\n",
      "/home/bandheyh/common/survival-lcs/pipeline/simulated_datasets/EDM-1_one_of_each/het_f100_maf0.2_EDM-1_01.txt\n",
      "het_f100_maf0.2\n",
      "None\n",
      "/home/bandheyh/common/survival-lcs/pipeline/simulated_datasets/EDM-1_one_of_each/het_f100_maf0.4_EDM-1_01.txt\n",
      "het_f100_maf0.4\n",
      "None\n",
      "/home/bandheyh/common/survival-lcs/pipeline/simulated_datasets/EDM-1_one_of_each/add_f100_maf0.2_EDM-1_01.txt\n",
      "add_f100_maf0.2\n",
      "None\n",
      "/home/bandheyh/common/survival-lcs/pipeline/simulated_datasets/EDM-1_one_of_each/add_f100_maf0.4_EDM-1_01.txt\n",
      "add_f100_maf0.4\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from survival_LCS_permutations import survivalLCS_permutations as survivalLCS\n",
    "job_obj_list = list()\n",
    "for i in range(0,len(models)):\n",
    "    for j in range(0,len(nfeat)):\n",
    "        brier_df_list = list()\n",
    "        for k in range(0,len(maf)):\n",
    "            g, mtype, d, m, o, e,brier_df,cox_brier_df, m0_path, m0_type, m1_path, m1_type = get_parameters(models, nfeat, maf, i, j, k)\n",
    "            slcs = survivalLCS(g, mtype, d, m, o, e,brier_df,cox_brier_df, m0_path, m0_type, m1_path, m1_type, \n",
    "                                      c = c,iterations = iterations, cv = cv_splits)\n",
    "            if HPC == False:\n",
    "                brier_df = run_slcs(slcs)\n",
    "                brier_df_list.append(brier_df)\n",
    "            else:\n",
    "                job_obj_list.append(slcs)\n",
    "        if HPC == False:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPC Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster, LSFCluster, SGECluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster(cluster_type='SLURM', output_path=\".\", queue='defq', memory=4):\n",
    "    client = None\n",
    "    try:\n",
    "        if cluster_type == 'SLURM':\n",
    "            cluster = SLURMCluster(queue=queue,\n",
    "                                   cores=1,\n",
    "                                   memory=str(memory) + \"G\",\n",
    "                                   walltime=\"24:00:00\",\n",
    "                                   log_directory=output_path + \"/dask_logs/\")\n",
    "            cluster.adapt(maximum_jobs=400)\n",
    "        elif cluster_type == \"LSF\":\n",
    "            cluster = LSFCluster(queue=queue,\n",
    "                                 cores=1,\n",
    "                                 mem=memory * 1000000000,\n",
    "                                 memory=str(memory) + \"G\",\n",
    "                                 walltime=\"24:00\",\n",
    "                                 log_directory=output_path + \"/dask_logs/\")\n",
    "            cluster.adapt(maximum_jobs=400)\n",
    "        elif cluster_type == 'UGE':\n",
    "            cluster = SGECluster(queue=queue,\n",
    "                                 cores=1,\n",
    "                                 memory=str(memory) + \"G\",\n",
    "                                 resource_spec=\"mem_free=\" + str(memory) + \"G\",\n",
    "                                 walltime=\"24:00:00\",\n",
    "                                 log_directory=output_path + \"/dask_logs/\")\n",
    "            cluster.adapt(maximum_jobs=400)\n",
    "        elif cluster_type == 'Local':\n",
    "            c = Client()\n",
    "            cluster = c.cluster\n",
    "        else:\n",
    "            raise Exception(\"Unknown or Unsupported Cluster Type\")\n",
    "        client = Client(cluster)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise Exception(\"Exception: Unknown Exception\")\n",
    "    print(\"Running dask-cluster\")\n",
    "    print(client.scheduler_info())\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster = get_cluster(output_path=homedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel(model):\n",
    "    try:\n",
    "        brier_df = run_slcs(model)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        brier_df = e\n",
    "    return brier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<survival_LCS_permutations.survivalLCS_permutations at 0x15550db25b80>,\n",
       " <survival_LCS_permutations.survivalLCS_permutations at 0x15550db254f0>,\n",
       " <survival_LCS_permutations.survivalLCS_permutations at 0x15550db251f0>,\n",
       " <survival_LCS_permutations.survivalLCS_permutations at 0x1555126c2910>,\n",
       " <survival_LCS_permutations.survivalLCS_permutations at 0x15550db25100>,\n",
       " <survival_LCS_permutations.survivalLCS_permutations at 0x15550db16970>,\n",
       " <survival_LCS_permutations.survivalLCS_permutations at 0x155524d47310>,\n",
       " <survival_LCS_permutations.survivalLCS_permutations at 0x15550db41ac0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_obj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HPC == True:\n",
    "    delayed_results = []\n",
    "    for model in job_obj_list:\n",
    "        brier_df = dask.delayed(run_parallel)(model)\n",
    "        delayed_results.append(brier_df)\n",
    "    results = dask.compute(*delayed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if HPC:\n",
    "#     results = dask.compute([dask.delayed(run_parallel)(model) for model in job_obj_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(homedir+'/results_perm_final.pkl', 'wb') as file:\n",
    "    pickle.dump(results, file, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_idxs = list()\n",
    "for i in range(len(results)):\n",
    "    if type(results[i]) ==  ValueError:\n",
    "        print(i, results[i])\n",
    "        error_idxs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.arange(len(results)).reshape(len(models), len(nfeat), len(maf))\n",
    "\n",
    "# Convert a 1D index to a 3D index\n",
    "for x in error_idxs:\n",
    "    i, j, k = np.unravel_index(x, arr.shape)\n",
    "    print(models[i], nfeat[j], maf[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBS Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "brier_df_list = list()\n",
    "arr = np.arange(len(results)).reshape(len(models), len(nfeat), len(maf))\n",
    "for x in range(len(results)):\n",
    "    i, j, k = np.unravel_index(x, arr.shape)\n",
    "    print(models[i], nfeat[j], maf[k])\n",
    "    current_ibs = results[x]\n",
    "    # current_ibs = current_ibs.rename(columns={\"mean\": str(models[i])+'_'+str(nfeat[j])+'_'+str(maf[k]), \n",
    "    #                                         \"ci_lower\": str(models[i])+'_'+str(nfeat[j])+'_'+str(maf[k])+'_ci_lower', \n",
    "    #                                         \"ci_upper\": str(models[i])+'_'+str(nfeat[j])+'_'+str(maf[k])+'_ci_upper'})\n",
    "    brier_df_list.append(current_ibs)\n",
    "brier_df = pd.concat(brier_df_list, axis = 1, sort = False).reset_index()\n",
    "#print('brier_df:', brier_df)\n",
    "brier_df.to_csv(homedir+'/perm_ibs_data_all.csv', index = False)\n",
    "brier_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slcs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
