{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SurvivalLCS Experiment Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import glob\n",
    "from datetime import date\n",
    "import argparse\n",
    "from random import shuffle\n",
    "from random import sample\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import shutil\n",
    "import sksurv\n",
    "import pickle\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from survival_LCS_permutations import survivalLCS_permutations as survivalLCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/bandheyh/common/survival-lcs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x15550c87d490>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.ioff()\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survival-LCS Parameters\n",
    "\n",
    "### Set file names and necessary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter to run using hpc resources\n",
    "HPC = True\n",
    "\n",
    "homedir = \"/home/bandheyh/common/survival-lcs/pipeline\"\n",
    "models = ['me', 'epi', 'het', 'add']\n",
    "m0s = []\n",
    "\n",
    "c = [0.1,0.4,0.8]\n",
    "nfeat = ['f100'] #add f10000 when on cluster\n",
    "maf = ['maf0.2','maf0.4']\n",
    "\n",
    "iterations = 50000\n",
    "cv_splits = 5\n",
    "\n",
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    models = ['me']\n",
    "    c = [0.1]\n",
    "    nfeat = ['f100', 'f1000']\n",
    "    maf = ['maf0.2', 'maf0.4']\n",
    "    iterations = 1000\n",
    "    cv_splits = 3\n",
    "\n",
    "### Create empty brier score DataFrame\n",
    "brier_df = pd.DataFrame()\n",
    "cox_brier_df = pd.DataFrame()\n",
    "\n",
    "# other non-parameters\n",
    "\n",
    "simulated = True # CHANGE THIS TO FALSE IF RUNNING REAL DATA\n",
    "\n",
    "lcs_run = True\n",
    "dtype_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the survival_LCS pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from survival_LCS_permutations import survivalLCS_permutations as survivalLCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the survival_LCS pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(models, nfeat, maf, i, j, k):\n",
    "\n",
    "    g = homedir + '/' + 'simulated_datasets/' + \\\n",
    "        'EDM-1_one_of_each/'+str(models[i]) + \\\n",
    "        '_' + str(nfeat[j]) + '_' + str(maf[k]) + '_' + 'EDM-1_01.txt'\n",
    "    dtype = str(models[i]) + '_' + str(nfeat[j]) + '_' + str(maf[k])\n",
    "    dtype_list.append(dtype)\n",
    "    print(g)\n",
    "\n",
    "    d = homedir + '/' + 'cv_sim_data/cv_' + str(models[i]) + '/' + dtype\n",
    "    m = homedir + '/' + 'pickled_cv_models/' + str(models[i]) + '/' + dtype\n",
    "    o = homedir + '/' + 'sim_lcs_output/' + str(models[i]) + '/' + dtype\n",
    "\n",
    "    ### Set m0_path\n",
    "    if models[i] in ['me','add','het']:\n",
    "        m0_path = homedir+'/'+'simulated_datasets/'+'EDM-1_one_of_each/model_files/me_h0.2_'+str(maf[k])+'_Models.txt'\n",
    "    else:\n",
    "        m0_path = homedir+'/'+'simulated_datasets/'+'EDM-1_one_of_each/model_files/epi_h0.2_'+str(maf[k])+'_Models.txt'\n",
    "\n",
    "    ### Set m1_path\n",
    "    if models[i] in ['me','epi']:\n",
    "        m1_path = None\n",
    "    else:\n",
    "        m1_path = homedir+'/'+'simulated_datasets/'+'EDM-1_one_of_each/model_files/epi_h0.2_'+str(maf[k])+'_Models.txt'\n",
    "\n",
    "    ### Set m0_type\n",
    "    if models[i] in ['me','add','het']:\n",
    "        m0_type = 'main_effect'\n",
    "    else:\n",
    "        m0_type = '2way_epistasis'\n",
    "\n",
    "    ### Set m1_type\n",
    "    if models[i] in ['me', 'epi']:\n",
    "        m1_type = None\n",
    "    else:\n",
    "        m1_type = '2way_epistasis'\n",
    "\n",
    "    ### Set mtype\n",
    "    if models[i] == 'me':\n",
    "        mtype = 'main_effect'\n",
    "    elif models[i] == 'epi':\n",
    "        mtype = '2way_epistasis'\n",
    "    elif models[i] == 'add':\n",
    "        mtype = 'additive'\n",
    "    else:\n",
    "        mtype = 'heterogeneous'\n",
    "\n",
    "\n",
    "    e = \"testallsims\"\n",
    "    print(str(models[i])+'_'+str(nfeat[j])+'_'+str(maf[k]))\n",
    "\n",
    "    return g, mtype, d, m, o, e,brier_df,cox_brier_df, m0_path, m0_type, m1_path, m1_type\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_slcs(survivalLCS):\n",
    "\n",
    "    lcs_run = True\n",
    "\n",
    "    if lcs_run == True:\n",
    "        survivalLCS.returnCVModelFiles()\n",
    "        brier_df = survivalLCS.returnIBSresults()\n",
    "\n",
    "    else:\n",
    "        print(\"Datasets generated only\")\n",
    "\n",
    "    return brier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bandheyh/common/survival-lcs/pipeline/simulated_datasets/EDM-1_one_of_each/me_f100_maf0.2_EDM-1_01.txt\n",
      "me_f100_maf0.2\n",
      "None\n",
      "/home/bandheyh/common/survival-lcs/pipeline/simulated_datasets/EDM-1_one_of_each/me_f100_maf0.4_EDM-1_01.txt\n",
      "me_f100_maf0.4\n",
      "None\n",
      "/home/bandheyh/common/survival-lcs/pipeline/simulated_datasets/EDM-1_one_of_each/epi_f100_maf0.2_EDM-1_01.txt\n",
      "epi_f100_maf0.2\n",
      "None\n",
      "/home/bandheyh/common/survival-lcs/pipeline/simulated_datasets/EDM-1_one_of_each/epi_f100_maf0.4_EDM-1_01.txt\n",
      "epi_f100_maf0.4\n",
      "None\n",
      "/home/bandheyh/common/survival-lcs/pipeline/simulated_datasets/EDM-1_one_of_each/het_f100_maf0.2_EDM-1_01.txt\n",
      "het_f100_maf0.2\n",
      "None\n",
      "/home/bandheyh/common/survival-lcs/pipeline/simulated_datasets/EDM-1_one_of_each/het_f100_maf0.4_EDM-1_01.txt\n",
      "het_f100_maf0.4\n",
      "None\n",
      "/home/bandheyh/common/survival-lcs/pipeline/simulated_datasets/EDM-1_one_of_each/add_f100_maf0.2_EDM-1_01.txt\n",
      "add_f100_maf0.2\n",
      "None\n",
      "/home/bandheyh/common/survival-lcs/pipeline/simulated_datasets/EDM-1_one_of_each/add_f100_maf0.4_EDM-1_01.txt\n",
      "add_f100_maf0.4\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from survival_LCS_permutations import survivalLCS_permutations as survivalLCS\n",
    "job_obj_list = list()\n",
    "for i in range(0,len(models)):\n",
    "    for j in range(0,len(nfeat)):\n",
    "        brier_df_list = list()\n",
    "        for k in range(0,len(maf)):\n",
    "            g, mtype, d, m, o, e,brier_df,cox_brier_df, m0_path, m0_type, m1_path, m1_type = get_parameters(models, nfeat, maf, i, j, k)\n",
    "            slcs = survivalLCS(g, mtype, d, m, o, e,brier_df,cox_brier_df, m0_path, m0_type, m1_path, m1_type, \n",
    "                                      c = c,iterations = iterations, cv = cv_splits)\n",
    "            if HPC == False:\n",
    "                brier_df = run_slcs(slcs)\n",
    "                brier_df_list.append(brier_df)\n",
    "            else:\n",
    "                job_obj_list.append(slcs)\n",
    "        if HPC == False:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPC Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster, LSFCluster, SGECluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster(cluster_type='SLURM', output_path=\".\", queue='defq', memory=4):\n",
    "    client = None\n",
    "    try:\n",
    "        if cluster_type == 'SLURM':\n",
    "            cluster = SLURMCluster(queue=queue,\n",
    "                                   cores=1,\n",
    "                                   memory=str(memory) + \"G\",\n",
    "                                   walltime=\"24:00:00\",\n",
    "                                   log_directory=output_path + \"/dask_logs/\")\n",
    "            cluster.adapt(maximum_jobs=400)\n",
    "        elif cluster_type == \"LSF\":\n",
    "            cluster = LSFCluster(queue=queue,\n",
    "                                 cores=1,\n",
    "                                 mem=memory * 1000000000,\n",
    "                                 memory=str(memory) + \"G\",\n",
    "                                 walltime=\"24:00\",\n",
    "                                 log_directory=output_path + \"/dask_logs/\")\n",
    "            cluster.adapt(maximum_jobs=400)\n",
    "        elif cluster_type == 'UGE':\n",
    "            cluster = SGECluster(queue=queue,\n",
    "                                 cores=1,\n",
    "                                 memory=str(memory) + \"G\",\n",
    "                                 resource_spec=\"mem_free=\" + str(memory) + \"G\",\n",
    "                                 walltime=\"24:00:00\",\n",
    "                                 log_directory=output_path + \"/dask_logs/\")\n",
    "            cluster.adapt(maximum_jobs=400)\n",
    "        elif cluster_type == 'Local':\n",
    "            c = Client()\n",
    "            cluster = c.cluster\n",
    "        else:\n",
    "            raise Exception(\"Unknown or Unsupported Cluster Type\")\n",
    "        client = Client(cluster)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise Exception(\"Exception: Unknown Exception\")\n",
    "    print(\"Running dask-cluster\")\n",
    "    print(client.scheduler_info())\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cluster \u001b[38;5;241m=\u001b[39m \u001b[43mget_cluster\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhomedir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 5\u001b[0m, in \u001b[0;36mget_cluster\u001b[0;34m(cluster_type, output_path, queue, memory)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cluster_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSLURM\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 5\u001b[0m         cluster \u001b[38;5;241m=\u001b[39m \u001b[43mSLURMCluster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mcores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mG\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mwalltime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m24:00:00\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mlog_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/dask_logs/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m         cluster\u001b[38;5;241m.\u001b[39madapt(maximum_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m cluster_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSF\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/common/anaconda3/envs/slcs/lib/python3.9/site-packages/dask_jobqueue/core.py:654\u001b[0m, in \u001b[0;36mJobQueueCluster.__init__\u001b[0;34m(self, n_workers, job_cls, loop, security, shared_temp_directory, silence_logs, name, asynchronous, dashboard_address, host, scheduler_options, scheduler_cls, interface, protocol, config_name, **job_kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m     worker[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    649\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_job_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocesses\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    650\u001b[0m     ]\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dummy_job  \u001b[38;5;66;03m# trigger property to ensure that the job is valid\u001b[39;00m\n\u001b[0;32m--> 654\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43msecurity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msecurity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43msilence_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilence_logs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43masynchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_workers:\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale(n_workers)\n",
      "File \u001b[0;32m~/common/anaconda3/envs/slcs/lib/python3.9/site-packages/distributed/deploy/spec.py:284\u001b[0m, in \u001b[0;36mSpecCluster.__init__\u001b[0;34m(self, workers, scheduler, worker, asynchronous, loop, security, silence_logs, name, shutdown_on_close, scheduler_sync_interval)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalled_from_running_loop:\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop_runner\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_correct_state)\n",
      "File \u001b[0;32m~/common/anaconda3/envs/slcs/lib/python3.9/site-packages/distributed/utils.py:358\u001b[0m, in \u001b[0;36mSyncMethodMixin.sync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/common/anaconda3/envs/slcs/lib/python3.9/site-packages/distributed/utils.py:431\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m e\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[0;32m--> 431\u001b[0m         \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "File \u001b[0;32m~/common/anaconda3/envs/slcs/lib/python3.9/site-packages/distributed/utils.py:420\u001b[0m, in \u001b[0;36msync.<locals>.wait\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m         loop\u001b[38;5;241m.\u001b[39madd_callback(cancel)\n",
      "File \u001b[0;32m~/common/anaconda3/envs/slcs/lib/python3.9/threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 581\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/common/anaconda3/envs/slcs/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cluster = get_cluster(output_path=homedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel(model):\n",
    "    try:\n",
    "        brier_df = run_slcs(model)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        brier_df = e\n",
    "    return brier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<survival_LCS_permutations.survivalLCS_permutations at 0x15550db25b80>,\n",
       " <survival_LCS_permutations.survivalLCS_permutations at 0x15550db254f0>,\n",
       " <survival_LCS_permutations.survivalLCS_permutations at 0x15550db251f0>,\n",
       " <survival_LCS_permutations.survivalLCS_permutations at 0x1555126c2910>,\n",
       " <survival_LCS_permutations.survivalLCS_permutations at 0x15550db25100>,\n",
       " <survival_LCS_permutations.survivalLCS_permutations at 0x15550db16970>,\n",
       " <survival_LCS_permutations.survivalLCS_permutations at 0x155524d47310>,\n",
       " <survival_LCS_permutations.survivalLCS_permutations at 0x15550db41ac0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_obj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HPC == True:\n",
    "    delayed_results = []\n",
    "    for model in job_obj_list:\n",
    "        brier_df = dask.delayed(run_parallel)(model)\n",
    "        delayed_results.append(brier_df)\n",
    "    results = dask.compute(*delayed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if HPC:\n",
    "#     results = dask.compute([dask.delayed(run_parallel)(model) for model in job_obj_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(homedir+'/results_perm_final.pkl', 'wb') as file:\n",
    "    pickle.dump(results, file, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_idxs = list()\n",
    "for i in range(len(results)):\n",
    "    if type(results[i]) ==  ValueError:\n",
    "        print(i, results[i])\n",
    "        error_idxs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.arange(len(results)).reshape(len(models), len(nfeat), len(maf))\n",
    "\n",
    "# Convert a 1D index to a 3D index\n",
    "for x in error_idxs:\n",
    "    i, j, k = np.unravel_index(x, arr.shape)\n",
    "    print(models[i], nfeat[j], maf[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBS Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "brier_df_list = list()\n",
    "arr = np.arange(len(results)).reshape(len(models), len(nfeat), len(maf))\n",
    "for x in range(len(results)):\n",
    "    i, j, k = np.unravel_index(x, arr.shape)\n",
    "    print(models[i], nfeat[j], maf[k])\n",
    "    current_ibs = results[x]\n",
    "    # current_ibs = current_ibs.rename(columns={\"mean\": str(models[i])+'_'+str(nfeat[j])+'_'+str(maf[k]), \n",
    "    #                                         \"ci_lower\": str(models[i])+'_'+str(nfeat[j])+'_'+str(maf[k])+'_ci_lower', \n",
    "    #                                         \"ci_upper\": str(models[i])+'_'+str(nfeat[j])+'_'+str(maf[k])+'_ci_upper'})\n",
    "    brier_df_list.append(current_ibs)\n",
    "brier_df = pd.concat(brier_df_list, axis = 1, sort = False).reset_index()\n",
    "#print('brier_df:', brier_df)\n",
    "brier_df.to_csv(homedir+'/perm_ibs_data_all.csv', index = False)\n",
    "brier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slcs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
