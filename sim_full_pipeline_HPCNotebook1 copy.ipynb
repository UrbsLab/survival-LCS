{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SurvivalLCS Experiment Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.2, Python 3.9.19)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import glob\n",
    "from datetime import date\n",
    "import argparse\n",
    "from random import shuffle\n",
    "from random import sample\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import shutil\n",
    "import sksurv\n",
    "import pickle\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from survival_LCS_pipeline import survivalLCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/bandheyh/common/survival-lcs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x15555102ef40>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.ioff()\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import py scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import survival_AttributeTracking\n",
    "import survival_Classifier\n",
    "import survival_ClassifierSet\n",
    "import survival_DataManagement\n",
    "import survival_ExpertKnowledge\n",
    "import survival_ExSTraCS\n",
    "import survival_IterationRecord\n",
    "import survival_Pareto\n",
    "import survival_Prediction\n",
    "import survival_StringEnumerator\n",
    "import survival_OfflineEnvironment\n",
    "import survival_Timer\n",
    "import survival_RuleCompaction\n",
    "import survival_Metrics\n",
    "import utils\n",
    "import nonparametric_estimators\n",
    "import importGametes\n",
    "import survival_data_simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test run scripts interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i survival_AttributeTracking.py\n",
    "%run -i survival_Classifier.py\n",
    "%run -i survival_ClassifierSet.py\n",
    "%run -i survival_DataManagement.py\n",
    "%run -i survival_ExpertKnowledge.py\n",
    "%run -i survival_ExSTraCS.py\n",
    "%run -i survival_IterationRecord.py\n",
    "%run -i survival_Pareto.py\n",
    "%run -i survival_Prediction.py\n",
    "%run -i survival_StringEnumerator.py\n",
    "%run -i survival_OfflineEnvironment.py\n",
    "%run -i survival_Timer.py\n",
    "%run -i survival_RuleCompaction.py\n",
    "%run -i survival_Metrics.py\n",
    "%run -i utils.py\n",
    "%run -i nonparametric_estimators.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survival-LCS Parameters\n",
    "\n",
    "### Set file names and necessary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter to run using hpc resources\n",
    "HPC = True\n",
    "\n",
    "homedir = \"/home/bandheyh/common/survival-lcs/pipeline_copy\"\n",
    "models = ['me', 'epi', 'het', 'add']\n",
    "m0s = []\n",
    "\n",
    "c = [0.1,0.4,0.8]\n",
    "nfeat = ['f100','f1000', 'f10000'] #add f10000 when on cluster\n",
    "maf = ['maf0.2','maf0.4']\n",
    "\n",
    "iterations = 50000\n",
    "cv_splits = 5\n",
    "\n",
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    models = ['me']\n",
    "    c = [0.1]\n",
    "    nfeat = ['f100', 'f1000']\n",
    "    maf = ['maf0.2', 'maf0.4']\n",
    "    iterations = 1000\n",
    "    cv_splits = 3\n",
    "\n",
    "### Create empty brier score DataFrame\n",
    "brier_df = pd.DataFrame()\n",
    "cox_brier_df = pd.DataFrame()\n",
    "\n",
    "# other non-parameters\n",
    "\n",
    "simulated = True # CHANGE THIS TO FALSE IF RUNNING REAL DATA\n",
    "\n",
    "lcs_run = True\n",
    "dtype_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the survival_LCS pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from survival_LCS_pipeline import survivalLCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the directory structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to create the following folders and subfolders, INSIDE of the home directory for output files:\n",
    "1. cv_sim_data (with subfolders: cv_me, cv_epi, cv_het, cv_add)\n",
    "2. pickled_cv_models (with subfolders: me, epi, het, add)\n",
    "3. sim_lcs_output (with subfolders: me, epi, het, add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_folder(path, overwrite=False):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    else:\n",
    "        if overwrite:\n",
    "            shutil.rmtree(path)\n",
    "            os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_folder_structure(homedir, models, overwrite=True):\n",
    "    if overwrite==True:\n",
    "        make_folder(homedir+'/cv_sim_data/')\n",
    "        make_folder(homedir+'/pickled_cv_models/')\n",
    "        make_folder(homedir+'/sim_lcs_output/')\n",
    "        for model in models:\n",
    "            make_folder(homedir+'/cv_sim_data/cv_' + model, overwrite=overwrite)\n",
    "            make_folder(homedir+'/pickled_cv_models/' + model, overwrite=overwrite)\n",
    "            make_folder(homedir+'/sim_lcs_output/' + model, overwrite=overwrite)\n",
    "    else:\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_folder_structure(homedir, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the survival_LCS pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(models, nfeat, maf, i, j, k):\n",
    "\n",
    "    g = homedir + '/' + 'simulated_datasets/' + \\\n",
    "        'EDM-1_one_of_each/'+str(models[i]) + \\\n",
    "        '_' + str(nfeat[j]) + '_' + str(maf[k]) + '_' + 'EDM-1_01.txt'\n",
    "    dtype = str(models[i]) + '_' + str(nfeat[j]) + '_' + str(maf[k])\n",
    "    dtype_list.append(dtype)\n",
    "    print(g)\n",
    "\n",
    "    d = homedir + '/' + 'cv_sim_data/cv_' + str(models[i]) + '/' + dtype\n",
    "    m = homedir + '/' + 'pickled_cv_models/' + str(models[i]) + '/' + dtype\n",
    "    o = homedir + '/' + 'sim_lcs_output/' + str(models[i]) + '/' + dtype\n",
    "\n",
    "    ### Set m0_path\n",
    "    if models[i] in ['me','add','het']:\n",
    "        m0_path = homedir+'/'+'simulated_datasets/'+'EDM-1_one_of_each/model_files/me_h0.2_'+str(maf[k])+'_Models.txt'\n",
    "    else:\n",
    "        m0_path = homedir+'/'+'simulated_datasets/'+'EDM-1_one_of_each/model_files/epi_h0.2_'+str(maf[k])+'_Models.txt'\n",
    "\n",
    "    ### Set m1_path\n",
    "    if models[i] in ['me','epi']:\n",
    "        m1_path = None\n",
    "    else:\n",
    "        m1_path = homedir+'/'+'simulated_datasets/'+'EDM-1_one_of_each/model_files/epi_h0.2_'+str(maf[k])+'_Models.txt'\n",
    "\n",
    "    ### Set m0_type\n",
    "    if models[i] in ['me','add','het']:\n",
    "        m0_type = 'main_effect'\n",
    "    else:\n",
    "        m0_type = '2way_epistasis'\n",
    "\n",
    "    ### Set m1_type\n",
    "    if models[i] in ['me', 'epi']:\n",
    "        m1_type = None\n",
    "    else:\n",
    "        m1_type = '2way_epistasis'\n",
    "\n",
    "    ### Set mtype\n",
    "    if models[i] == 'me':\n",
    "        mtype = 'main_effect'\n",
    "    elif models[i] == 'epi':\n",
    "        mtype = '2way_epistasis'\n",
    "    elif models[i] == 'add':\n",
    "        mtype = 'additive'\n",
    "    else:\n",
    "        mtype = 'heterogeneous'\n",
    "\n",
    "\n",
    "    e = \"testallsims\"\n",
    "    print(str(models[i])+'_'+str(nfeat[j])+'_'+str(maf[k]))\n",
    "\n",
    "    return g, mtype, d, m, o, e,brier_df,cox_brier_df, m0_path, m0_type, m1_path, m1_type\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_slcs(survivalLCS):\n",
    "    survivalLCS.returnPenetrance()\n",
    "    survivalLCS.returnSurvivalData()\n",
    "\n",
    "    lcs_run = True\n",
    "\n",
    "    if lcs_run == True:\n",
    "        survivalLCS.returnCVDatasets()\n",
    "        survivalLCS.returnCVModelFiles()\n",
    "\n",
    "        current_ibs = survivalLCS.returnIBSresults()\n",
    "        # current_ibs = current_ibs.rename(columns={\"mean\": str(models[i])+'_'+str(nfeat[j])+'_'+str(maf[k]), \n",
    "        #                                           \"ci_lower\": str(models[i])+'_'+str(nfeat[j])+'_'+str(maf[k])+'_ci_lower', \n",
    "        #                                           \"ci_upper\": str(models[i])+'_'+str(nfeat[j])+'_'+str(maf[k])+'_ci_upper'})\n",
    "    else:\n",
    "        print(\"Datasets generated only\")\n",
    "\n",
    "    print(survivalLCS.model_type)\n",
    "\n",
    "    return current_ibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_breir_output(brier_df_list, output_path, model_type, models, dtype_list, i):\n",
    "    brier_df = pd.concat(brier_df_list, axis = 1, sort = False).reset_index()\n",
    "\n",
    "    brier_df.to_csv(homedir +'/'+'sim_lcs_output/'+str(models[i])+'/ibs_data_'+mtype+'.txt', index = False)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Brier score')\n",
    "    plt.ylim(0,1)\n",
    "\n",
    "    for i in range(1,len(dtype_list)):\n",
    "        plt.plot(brier['times'], brier[dtype_list[i]],label = brier[dtype_list[i]].name)\n",
    "        plt.fill_between(brier['times'], brier[dtype_list[i]+'_ci_lower'], brier[dtype_list[i]+'_ci_upper'], color='b', alpha=.1)\n",
    "    plt.savefig(output_path+'/brier_scores_'+model_type + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from survival_LCS_pipeline import survivalLCS\n",
    "job_obj_list = list()\n",
    "for i in range(0,len(models)):\n",
    "    for j in range(0,len(nfeat)):\n",
    "        brier_df_list = list()\n",
    "        for k in range(0,len(maf)):\n",
    "            g, mtype, d, m, o, e,brier_df,cox_brier_df, m0_path, m0_type, m1_path, m1_type = get_parameters(models, nfeat, maf, i, j, k)\n",
    "            slcs = survivalLCS(g, mtype, d, m, o, e,brier_df,cox_brier_df, m0_path, m0_type, m1_path, m1_type, \n",
    "                                      c = c,iterations = iterations, cv = cv_splits)\n",
    "            if HPC == False:\n",
    "                current_ibs = run_slcs(slcs)\n",
    "                brier_df_list.append(current_ibs)\n",
    "            else:\n",
    "                job_obj_list.append(slcs)\n",
    "        if HPC == False:\n",
    "            if lcs_run == True:\n",
    "                make_breir_output(brier_df_list, survivalLCS.output_path, survivalLCS.model_type, models, dtype_list, i)\n",
    "            else:\n",
    "                print('LCS not run, no brier scores available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPC Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster, LSFCluster, SGECluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster(cluster_type='SLURM', output_path=\".\", queue='defq', memory=16):\n",
    "    client = None\n",
    "    try:\n",
    "        if cluster_type == 'SLURM':\n",
    "            cluster = SLURMCluster(queue=queue,\n",
    "                                   cores=1,\n",
    "                                   memory=str(memory) + \"G\",\n",
    "                                   walltime=\"24:00:00\",\n",
    "                                   log_directory=output_path + \"/dask_logs/\")\n",
    "            cluster.adapt(maximum_jobs=400)\n",
    "        elif cluster_type == \"LSF\":\n",
    "            cluster = LSFCluster(queue=queue,\n",
    "                                 cores=1,\n",
    "                                 mem=memory * 1000000000,\n",
    "                                 memory=str(memory) + \"G\",\n",
    "                                 walltime=\"24:00\",\n",
    "                                 log_directory=output_path + \"/dask_logs/\")\n",
    "            cluster.adapt(maximum_jobs=400)\n",
    "        elif cluster_type == 'UGE':\n",
    "            cluster = SGECluster(queue=queue,\n",
    "                                 cores=1,\n",
    "                                 memory=str(memory) + \"G\",\n",
    "                                 resource_spec=\"mem_free=\" + str(memory) + \"G\",\n",
    "                                 walltime=\"24:00:00\",\n",
    "                                 log_directory=output_path + \"/dask_logs/\")\n",
    "            cluster.adapt(maximum_jobs=400)\n",
    "        elif cluster_type == 'Local':\n",
    "            c = Client()\n",
    "            cluster = c.cluster\n",
    "        else:\n",
    "            raise Exception(\"Unknown or Unsupported Cluster Type\")\n",
    "        client = Client(cluster)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise Exception(\"Exception: Unknown Exception\")\n",
    "    print(\"Running dask-cluster\")\n",
    "    print(client.scheduler_info())\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running dask-cluster\n",
      "{'type': 'Scheduler', 'id': 'Scheduler-08bb87c5-c6fa-4119-9d0a-deb5bc1f44a0', 'address': 'tcp://10.17.134.112:34079', 'services': {'dashboard': 36083}, 'started': 1712470152.9837692, 'workers': {}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bandheyh/common/anaconda3/envs/slcs/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 36083 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cluster = get_cluster(output_path=homedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_folder(homedir+'/dask_logs/', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel(model):\n",
    "    try:\n",
    "        brier_df = run_slcs(model)\n",
    "    except Exception as e:\n",
    "        brier_df = e\n",
    "    return brier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<survival_LCS_pipeline.survivalLCS at 0x15550dae4430>,\n",
       " <survival_LCS_pipeline.survivalLCS at 0x15550db37f10>,\n",
       " <survival_LCS_pipeline.survivalLCS at 0x15550dae4490>,\n",
       " <survival_LCS_pipeline.survivalLCS at 0x15550db37670>,\n",
       " <survival_LCS_pipeline.survivalLCS at 0x15550db43940>,\n",
       " <survival_LCS_pipeline.survivalLCS at 0x15550daf1df0>,\n",
       " <survival_LCS_pipeline.survivalLCS at 0x15550dae4460>,\n",
       " <survival_LCS_pipeline.survivalLCS at 0x15550daf1d90>,\n",
       " <survival_LCS_pipeline.survivalLCS at 0x15551290f910>,\n",
       " <survival_LCS_pipeline.survivalLCS at 0x15550db122e0>,\n",
       " <survival_LCS_pipeline.survivalLCS at 0x15550daf1970>,\n",
       " <survival_LCS_pipeline.survivalLCS at 0x15550db13970>,\n",
       " <survival_LCS_pipeline.survivalLCS at 0x15550daf2040>,\n",
       " <survival_LCS_pipeline.survivalLCS at 0x15550db07430>,\n",
       " <survival_LCS_pipeline.survivalLCS at 0x15550db13190>,\n",
       " <survival_LCS_pipeline.survivalLCS at 0x15550dad3c70>,\n",
       " <survival_LCS_pipeline.survivalLCS at 0x15550db07040>,\n",
       " <survival_LCS_pipeline.survivalLCS at 0x15550db4de80>,\n",
       " <survival_LCS_pipeline.survivalLCS at 0x15555017a9a0>,\n",
       " <survival_LCS_pipeline.survivalLCS at 0x15550dae9640>,\n",
       " <survival_LCS_pipeline.survivalLCS at 0x15550dae9130>,\n",
       " <survival_LCS_pipeline.survivalLCS at 0x15550db48370>,\n",
       " <survival_LCS_pipeline.survivalLCS at 0x15550db488e0>,\n",
       " <survival_LCS_pipeline.survivalLCS at 0x15550db02610>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_obj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HPC == True:\n",
    "    delayed_results = []\n",
    "    for model in job_obj_list:\n",
    "        brier_df = dask.delayed(run_parallel)(model)\n",
    "        delayed_results.append(brier_df)\n",
    "    results = dask.compute(*delayed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if HPC:\n",
    "#     results = dask.compute([dask.delayed(run_parallel)(model) for model in job_obj_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(homedir+'/results.pkl', 'wb') as file:\n",
    "    pickle.dump(results, file, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_idxs = list()\n",
    "for i in range(len(results)):\n",
    "    if type(results[i]) ==  ValueError:\n",
    "        print(i, results[i])\n",
    "        error_idxs.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.arange(len(results)).reshape(len(models), len(nfeat), len(maf))\n",
    "\n",
    "# Convert a 1D index to a 3D index\n",
    "for x in error_idxs:\n",
    "    i, j, k = np.unravel_index(x, arr.shape)\n",
    "    print(models[i], nfeat[j], maf[k])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slcs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
