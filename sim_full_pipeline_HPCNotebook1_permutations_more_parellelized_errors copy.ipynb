{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SurvivalLCS Experiment Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import glob\n",
    "from random import shuffle\n",
    "from random import sample\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import shutil\n",
    "import pickle\n",
    "from survival_ExSTraCS import ExSTraCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/bandheyh/common/survival-lcs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x155511fcca30>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.ioff()\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survival-LCS Parameters\n",
    "\n",
    "### Set file names and necessary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter to run using hpc resources\n",
    "HPC = True\n",
    "\n",
    "homedir = \"/home/bandheyh/common/survival-lcs/pipeline_copy\"\n",
    "models = ['me', 'epi', 'het', 'add']\n",
    "m0s = []\n",
    "\n",
    "c = [0.1,0.4,0.8]\n",
    "nfeat = ['f100'] #add f10000 when on cluster\n",
    "maf = ['maf0.2','maf0.4']\n",
    "\n",
    "iterations = 50000\n",
    "cv_splits = 5\n",
    "\n",
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    models = ['me']\n",
    "    c = [0.1]\n",
    "    nfeat = ['f100']\n",
    "    maf = ['maf0.2', 'maf0.4']\n",
    "    iterations = 1000\n",
    "    cv_splits = 3\n",
    "\n",
    "### Create empty brier score DataFrame\n",
    "brier_df = pd.DataFrame()\n",
    "cox_brier_df = pd.DataFrame()\n",
    "\n",
    "# other non-parameters\n",
    "\n",
    "simulated = True # CHANGE THIS TO FALSE IF RUNNING REAL DATA\n",
    "\n",
    "lcs_run = True\n",
    "dtype_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the survival_LCS pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(models, nfeat, maf, i, j, k):\n",
    "\n",
    "    g = homedir + '/' + 'simulated_datasets/' + \\\n",
    "        'EDM-1_one_of_each/'+str(models[i]) + \\\n",
    "        '_' + str(nfeat[j]) + '_' + str(maf[k]) + '_' + 'EDM-1_01.txt'\n",
    "    dtype = str(models[i]) + '_' + str(nfeat[j]) + '_' + str(maf[k])\n",
    "    dtype_list.append(dtype)\n",
    "    print(g)\n",
    "\n",
    "    d = homedir + '/' + 'cv_sim_data/cv_' + str(models[i]) + '/' + dtype\n",
    "    m = homedir + '/' + 'pickled_cv_models/' + str(models[i]) + '/' + dtype\n",
    "    o = homedir + '/' + 'sim_lcs_output/' + str(models[i]) + '/' + dtype\n",
    "\n",
    "    ### Set m0_path\n",
    "    if models[i] in ['me','add','het']:\n",
    "        m0_path = homedir+'/'+'simulated_datasets/'+'EDM-1_one_of_each/model_files/me_h0.2_'+str(maf[k])+'_Models.txt'\n",
    "    else:\n",
    "        m0_path = homedir+'/'+'simulated_datasets/'+'EDM-1_one_of_each/model_files/epi_h0.2_'+str(maf[k])+'_Models.txt'\n",
    "\n",
    "    ### Set m1_path\n",
    "    if models[i] in ['me','epi']:\n",
    "        m1_path = None\n",
    "    else:\n",
    "        m1_path = homedir+'/'+'simulated_datasets/'+'EDM-1_one_of_each/model_files/epi_h0.2_'+str(maf[k])+'_Models.txt'\n",
    "\n",
    "    ### Set m0_type\n",
    "    if models[i] in ['me','add','het']:\n",
    "        m0_type = 'main_effect'\n",
    "    else:\n",
    "        m0_type = '2way_epistasis'\n",
    "\n",
    "    ### Set m1_type\n",
    "    if models[i] in ['me', 'epi']:\n",
    "        m1_type = None\n",
    "    else:\n",
    "        m1_type = '2way_epistasis'\n",
    "\n",
    "    ### Set mtype\n",
    "    if models[i] == 'me':\n",
    "        mtype = 'main_effect'\n",
    "    elif models[i] == 'epi':\n",
    "        mtype = '2way_epistasis'\n",
    "    elif models[i] == 'add':\n",
    "        mtype = 'additive'\n",
    "    else:\n",
    "        mtype = 'heterogeneous'\n",
    "\n",
    "\n",
    "    e = \"testallsims\"\n",
    "    print(str(models[i])+'_'+str(nfeat[j])+'_'+str(maf[k]))\n",
    "\n",
    "    return g, mtype, d, m, o, e,brier_df,cox_brier_df, m0_path, m0_type, m1_path, m1_type\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from survival_ExSTraCS import ExSTraCS\n",
    "class ExSTraCSWraper:\n",
    "    def __init__(self, g, mtype, d, m, o, e, brier_df, cox_brier_df, \n",
    "                 m0_path = None, m0_type = None, m1_path = None, m1_type = None,\n",
    "                 c = 0.1, cv=0, perm_n = 0, T = 100, k = 8, \n",
    "                 time_label = \"eventTime\", status_label = \"eventStatus\", instance_label=\"inst\", \n",
    "                 random_state = None, iterations = 50000, \n",
    "                 nu = 1, rp = 1000):\n",
    "        self.gametes_data_path = g\n",
    "        self.gametes_model_path_0 = m0_path\n",
    "        self.gametes_model_path_1 = m1_path\n",
    "        self.data_path = d\n",
    "        self.model_path = m\n",
    "        self.output_path = o\n",
    "        self.experiment_name = e\n",
    "        self.model0_type = m0_type\n",
    "        self.model1_type = m1_type\n",
    "        self.model_type = mtype #add parameter with name of original dataset\n",
    "\n",
    "\n",
    "        self.time_label = time_label\n",
    "        self.status_label = status_label\n",
    "        self.instance_label = instance_label\n",
    "        self.T = T\n",
    "        self.knots = k\n",
    "        self.censor = c\n",
    "\n",
    "        self.iterations = iterations\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.cv_count = cv\n",
    "        self.nu = nu\n",
    "        self.rulepop = rp\n",
    "\n",
    "        if self.random_state == None:\n",
    "            self.random_state = random.randint(0, 1000000)\n",
    "        else:\n",
    "            self.random_state = int(self.random_state)\n",
    "        self.perm_n = perm_n\n",
    "\n",
    "\n",
    "    def fit(self):\n",
    "        train_file = self.data_path+ '/' + str(self.model_type) + '_cens'+ \\\n",
    "            str(self.censor) + '_surv_'+ str('2024-04-07') + '_CV_'+str(self.cv_count)+'_Train.txt'\n",
    "        data_train = pd.read_csv(train_file, sep='\\t') #, header = 0\n",
    "        timeLabel = self.time_label\n",
    "        censorLabel = self.status_label\n",
    "        instID = self.instance_label\n",
    "\n",
    "        # Derive the attribute and phenotype array using the phenotype label\n",
    "        dataFeatures_train = data_train.drop([timeLabel,censorLabel,instID],axis = 1).values\n",
    "        dataEvents_train = data_train[[timeLabel,censorLabel]].values\n",
    "\n",
    "        # split dataEvents into two separate arrays (time and censoring)\n",
    "        dataEventTimes_train = dataEvents_train[:,0]\n",
    "        dataEventStatus_train = dataEvents_train[:,1]\n",
    "\n",
    "\n",
    "        test_file = self.data_path+ '/' + str(self.model_type) + '_cens'+ str(self.censor)\\\n",
    "              + '_surv_'+ str('2024-04-07') + '_CV_'+str(self.cv_count)+'_Test.txt'\n",
    "        data_test = pd.read_csv(test_file, sep='\\t') #, headers = 0\n",
    "        timeLabel = self.time_label\n",
    "        censorLabel = self.status_label\n",
    "\n",
    "        np.random.shuffle(dataEventTimes_train)\n",
    "\n",
    "        ### Train the survival_ExSTraCS model\n",
    "        model = ExSTraCS(learning_iterations = self.iterations,nu=self.nu,N=self.rulepop)\n",
    "        self.trainedModel = model.fit(dataFeatures_train,dataEventTimes_train,dataEventStatus_train)\n",
    "\n",
    "    def brier_score(self):\n",
    "        train_file = self.data_path+ '/' + str(self.model_type) + '_cens'+ \\\n",
    "            str(self.censor) + '_surv_'+ str('2024-04-07') + '_CV_'+str(self.cv_count)+'_Train.txt'\n",
    "        data_train = pd.read_csv(train_file, sep='\\t') #, header = 0\n",
    "        timeLabel = self.time_label\n",
    "        censorLabel = self.status_label\n",
    "        instID = self.instance_label\n",
    "\n",
    "        #Derive the attribute and phenotype array using the phenotype label\n",
    "        dataFeatures_train = data_train.drop([timeLabel,censorLabel,instID],axis = 1).values\n",
    "        dataEvents_train = data_train[[timeLabel,censorLabel]].values\n",
    "\n",
    "        #split dataEvents into two separate arrays (time and censoring)\n",
    "        dataEventTimes_train = dataEvents_train[:,0]\n",
    "        dataEventStatus_train = dataEvents_train[:,1]\n",
    "\n",
    "\n",
    "        test_file = self.data_path+ '/' + str(self.model_type) + '_cens'+ str(self.censor)\\\n",
    "              + '_surv_'+ str('2024-04-07') + '_CV_'+str(self.cv_count)+'_Test.txt'\n",
    "        data_test = pd.read_csv(test_file, sep='\\t') #, headers = 0\n",
    "        timeLabel = self.time_label\n",
    "        censorLabel = self.status_label\n",
    "\n",
    "        #Derive the attribute and phenotype array using the phenotype label\n",
    "        dataFeatures_test = data_test.drop([timeLabel,censorLabel,instID],axis = 1).values\n",
    "        dataEvents_test = data_test[[timeLabel,censorLabel]].values\n",
    "\n",
    "        #split dataEvents into two separate arrays (time and censoring)\n",
    "        dataEventTimes_test = dataEvents_test[:,0]\n",
    "        dataEventStatus_test = dataEvents_test[:,1]\n",
    "        scoreEvents_train = np.flip(dataEvents_train, 1)\n",
    "        scoreEvents_test = np.flip(dataEvents_test, 1)\n",
    "\n",
    "        scoreEvents_train = np.core.records.fromarrays(scoreEvents_train.transpose(),names='cens, time', formats = '?, <f8')\n",
    "        scoreEvents_test = np.core.records.fromarrays(scoreEvents_test.transpose(),names='cens, time', formats = '?, <f8')\n",
    "\n",
    "\n",
    "        ### Convert float data to int\n",
    "        dataEventTimes_train = dataEventTimes_train.astype('int64')\n",
    "        dataEventTimes_test = dataEventTimes_test.astype('int64')\n",
    "        dataEventStatus_train = dataEventStatus_train.astype('int64')\n",
    "        dataEventStatus_test = dataEventStatus_test.astype('int64')\n",
    "\n",
    "        try:\n",
    "            times, b_scores = self.trainedModel.brier_score(dataFeatures_test, \n",
    "                                                            dataEventStatus_test,\n",
    "                                                            dataEventTimes_test, \n",
    "                                                            dataEventTimes_train,\n",
    "                                                            scoreEvents_train,\n",
    "                                                            scoreEvents_test)\n",
    "            tb = pd.DataFrame({'times': times, 'b_scores_' + \\\n",
    "                               str(os.path.basename(self.output_path)) + \\\n",
    "                                '_cens'+ str(self.censor) + \\\n",
    "                                    '_perm' + str(self.perm_n) + \\\n",
    "                                        '_cv' + str(self.cv_count): b_scores})\n",
    "            tb.to_csv(homedir + '/perm/' + str(os.path.basename(self.output_path)) + \\\n",
    "                                '_cens'+ str(self.censor) + \\\n",
    "                                    '_perm' + str(self.perm_n) + \\\n",
    "                                        '_cv' + str(self.cv_count) + '.csv', index=False)\n",
    "            tb.set_index('times',inplace=True)\n",
    "            return tb\n",
    "        except Exception as e:\n",
    "            return e\n",
    "\n",
    "    def return_ibs(self):\n",
    "        df = self.brier_score(self)\n",
    "        col_name = 'b_scores_' + \\\n",
    "                               str(os.path.basename(self.output_path)) + \\\n",
    "                                '_cens'+ str(self.censor) + \\\n",
    "                                    '_perm' + str(self.perm_n) + \\\n",
    "                                        '_cv' + str(self.cv_count)\n",
    "        temp_df = df[[col_name, 'times']]\n",
    "        temp_df = temp_df.dropna()\n",
    "        try:\n",
    "            val = np.trapz(temp_df[col_name], temp_df['times']) / (list(temp_df['times'])[-1] - list(temp_df['times'])[0])\n",
    "        except Exception as e:\n",
    "    #         print(col_name, e)\n",
    "            val = np.nan\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_n = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_idxs = [2056]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add f100 maf0.2 0.8 11 1\n",
      "/home/bandheyh/common/survival-lcs/pipeline_copy/simulated_datasets/EDM-1_one_of_each/add_f100_maf0.2_EDM-1_01.txt\n",
      "add_f100_maf0.2\n",
      "2 11 1\n"
     ]
    }
   ],
   "source": [
    "job_obj_list = list()\n",
    "arr = np.arange(len(models) * len(nfeat) * len(maf) * len(c) * \\\n",
    "                perm_n * cv_splits).reshape(len(models), len(nfeat), len(maf), len(c), perm_n, cv_splits)\n",
    "\n",
    "# Convert a 1D index to a 3D index\n",
    "for x in error_idxs:\n",
    "    i, j, k, ii, jj, kk = np.unravel_index(x, arr.shape)\n",
    "    print(models[i], nfeat[j], maf[k], c[ii], jj, kk)\n",
    "    g, mtype, d, m, o, e,brier_df,cox_brier_df, \\\n",
    "        m0_path, m0_type, m1_path, m1_type = get_parameters(models, nfeat, maf, i, j, k)\n",
    "    print(ii, jj, kk)\n",
    "    obj = ExSTraCSWraper(g, mtype, d, m, o, e, brier_df, cox_brier_df, \n",
    "                            m0_path, m0_type, m1_path, m1_type,\n",
    "                            c = c[ii], cv=kk, perm_n = jj)\n",
    "    job_obj_list.append(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPC Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster, LSFCluster, SGECluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster(cluster_type='SLURM', output_path=\".\", queue='defq', memory=4):\n",
    "    client = None\n",
    "    try:\n",
    "        if cluster_type == 'SLURM':\n",
    "            cluster = SLURMCluster(queue=queue,\n",
    "                                   cores=1,\n",
    "                                   memory=str(memory) + \"G\",\n",
    "                                   walltime=\"24:00:00\",\n",
    "                                   log_directory=output_path + \"/dask_logs/\")\n",
    "            cluster.adapt(maximum_jobs=400)\n",
    "        elif cluster_type == \"LSF\":\n",
    "            cluster = LSFCluster(queue=queue,\n",
    "                                 cores=1,\n",
    "                                 mem=memory * 1000000000,\n",
    "                                 memory=str(memory) + \"G\",\n",
    "                                 walltime=\"24:00\",\n",
    "                                 log_directory=output_path + \"/dask_logs/\")\n",
    "            cluster.adapt(maximum_jobs=400)\n",
    "        elif cluster_type == 'UGE':\n",
    "            cluster = SGECluster(queue=queue,\n",
    "                                 cores=1,\n",
    "                                 memory=str(memory) + \"G\",\n",
    "                                 resource_spec=\"mem_free=\" + str(memory) + \"G\",\n",
    "                                 walltime=\"24:00:00\",\n",
    "                                 log_directory=output_path + \"/dask_logs/\")\n",
    "            cluster.adapt(maximum_jobs=400)\n",
    "        elif cluster_type == 'Local':\n",
    "            c = Client()\n",
    "            cluster = c.cluster\n",
    "        else:\n",
    "            raise Exception(\"Unknown or Unsupported Cluster Type\")\n",
    "        client = Client(cluster)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise Exception(\"Exception: Unknown Exception\")\n",
    "    print(\"Running dask-cluster\")\n",
    "    print(client.scheduler_info())\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bandheyh/common/anaconda3/envs/slcs/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 37761 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running dask-cluster\n",
      "{'type': 'Scheduler', 'id': 'Scheduler-aecaa8e5-03f6-45a1-8f13-0364b9ebe653', 'address': 'tcp://10.17.134.112:39525', 'services': {'dashboard': 37761}, 'started': 1712863512.137886, 'workers': {}}\n"
     ]
    }
   ],
   "source": [
    "cluster = get_cluster(output_path=homedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel(modelwraper):\n",
    "    try:\n",
    "        modelwraper.fit()\n",
    "        brier_df = modelwraper.brier_score()\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        brier_df = e\n",
    "    return brier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.ExSTraCSWraper at 0x1555121a2bb0>, 1)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_obj_list[0], len(job_obj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HPC == True:\n",
    "    delayed_results = []\n",
    "    for model in job_obj_list:\n",
    "        brier_df = dask.delayed(run_parallel)(model)\n",
    "        delayed_results.append(brier_df)\n",
    "    results = dask.compute(*delayed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if HPC:\n",
    "#     results = dask.compute([dask.delayed(run_parallel)(model) for model in job_obj_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(homedir+'/results_perm_parallel_3.pkl', 'wb') as file:\n",
    "#     pickle.dump(results, file, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_idxs = list()\n",
    "for i in range(len(results)):\n",
    "    if type(results[i]) ==  ValueError:\n",
    "        print(i, results[i])\n",
    "        error_idxs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_idxs = list()\n",
    "for i in range(len(results)):\n",
    "    if type(results[i]) ==  ValueError:\n",
    "        print(i, results[i])\n",
    "        error_idxs.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBS Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brier_df_list = list()\n",
    "# arr = np.arange(len(results)).reshape(len(models), len(nfeat), len(maf), len(c), perm_n, cv_splits)\n",
    "# for x in range(len(results)):\n",
    "#     i, j, k, ii, jj, kk = np.unravel_index(x, arr.shape)\n",
    "#     print(models[i], nfeat[j], maf[k], c[ii], jj, kk)\n",
    "#     current_ibs = results[x]\n",
    "#     brier_df_list.append(current_ibs)\n",
    "# brier_df = pd.concat(brier_df_list, axis = 1, sort = False).reset_index()\n",
    "# brier_df.to_csv(homedir+'/perm_ibs_data_all_parallel.csv', index = False)\n",
    "# brier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_idxs_org = [2056]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(error_idxs_org)[error_idxs]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slcs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
